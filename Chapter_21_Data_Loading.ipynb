{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 21: Data Loading & Storage\n",
        "\n",
        "Reading and writing data in text, binary, web, and database formats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pandas I/O Overview (Slide 45)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p>Accessing data is the necessary first step for any analysis. pandas provides a rich set of I/O functions.</p>\n",
        "<p><strong>Reading Functions:</strong></p>\n",
        "<ul>\n",
        "<li><code>pd.read_csv()</code> \u2014 Comma-separated values (most common)</li>\n",
        "<li><code>pd.read_table()</code> \u2014 Tab-delimited (or custom delimiter)</li>\n",
        "<li><code>pd.read_excel()</code> \u2014 Excel XLS/XLSX files</li>\n",
        "<li><code>pd.read_json()</code> \u2014 JSON strings or files</li>\n",
        "<li><code>pd.read_html()</code> \u2014 Tables from HTML pages</li>\n",
        "<li><code>pd.read_sql()</code> \u2014 SQL query results</li>\n",
        "<li><code>pd.read_hdf()</code> \u2014 HDF5 binary format</li>\n",
        "<li><code>pd.read_pickle()</code> \u2014 Python pickle serialization</li>\n",
        "<li><code>pd.read_clipboard()</code> \u2014 Data copied to clipboard</li>\n",
        "</ul>\n",
        "<p><strong>Pattern:</strong> Every <code>read_*</code> function has a corresponding <code>to_*</code> method for writing data back.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** read_csv is the most-used function by far\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading CSV Files (Slide 46)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# pd.read_csv(filepath) \u2014 read comma-separated file into DataFrame\n",
        "# header=None       \u2014 file has no header row\n",
        "# names=[...]       \u2014 custom column names\n",
        "# index_col='col'   \u2014 use a column as the row index\n",
        "# sep='\\s+'         \u2014 custom delimiter (regex supported)\n",
        "\n",
        "# Basic read (first row = header)\n",
        "df = pd.read_csv('examples/ex1.csv')\n",
        "print(df)\n",
        "\n",
        "# No header row\n",
        "df = pd.read_csv('examples/ex2.csv', header=None)\n",
        "print(df)  # Columns named 0, 1, 2, ...\n",
        "\n",
        "# Custom column names\n",
        "df = pd.read_csv('examples/ex2.csv',\n",
        "                 names=['a', 'b', 'c', 'd', 'message'])\n",
        "\n",
        "# Use a column as index\n",
        "df = pd.read_csv('examples/ex2.csv',\n",
        "                 names=['a', 'b', 'c', 'd', 'message'],\n",
        "                 index_col='message')\n",
        "\n",
        "# Whitespace-separated (regex delimiter)\n",
        "df = pd.read_csv('examples/ex3.txt', sep='\\s+')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** read_csv is the workhorse function for tabular data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### read_csv: Key Parameters (Slide 47)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# skiprows=[0, 2, 3]   \u2014 skip specific rows\n",
        "# nrows=5              \u2014 read only first 5 rows\n",
        "# usecols=['a', 'b']   \u2014 read only specific columns\n",
        "# dtype={'col': str}   \u2014 force column data types\n",
        "# parse_dates=['date'] \u2014 parse column as datetime\n",
        "# encoding='utf-8'     \u2014 file encoding\n",
        "# comment='#'          \u2014 skip lines starting with #\n",
        "\n",
        "# Skip specific rows\n",
        "df = pd.read_csv('data.csv', skiprows=[0, 2, 3])\n",
        "\n",
        "# Read only first 5 rows (great for big files)\n",
        "df = pd.read_csv('big_data.csv', nrows=5)\n",
        "\n",
        "# Only load specific columns\n",
        "df = pd.read_csv('data.csv', usecols=['name', 'age', 'city'])\n",
        "\n",
        "# Force data types\n",
        "df = pd.read_csv('data.csv', dtype={'zipcode': str, 'id': int})\n",
        "\n",
        "# Parse dates automatically\n",
        "df = pd.read_csv('data.csv', parse_dates=['date_column'])\n",
        "print(df.dtypes)  # date_column is now datetime64\n",
        "\n",
        "# Handle different encodings\n",
        "df = pd.read_csv('data.csv', encoding='latin-1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Always check dtypes after reading \u2014 numbers stored as strings are common\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Missing Values in I/O (Slide 48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# na_values=['NA', 'NULL', 'missing'] \u2014 treat these strings as NaN\n",
        "# keep_default_na=True  \u2014 also treat default NA strings (empty, NA, NaN, etc.)\n",
        "# na_filter=True        \u2014 detect missing values (set False for speed)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Default: empty strings, 'NA', 'NaN', 'NULL' \u2192 NaN\n",
        "df = pd.read_csv('examples/ex5.csv')\n",
        "print(df)\n",
        "\n",
        "# Custom NA values\n",
        "df = pd.read_csv('examples/ex5.csv', na_values=['NULL', 'N/A'])\n",
        "\n",
        "# Different NA sentinels per column\n",
        "sentinels = {'message': ['foo', 'NA'],\n",
        "             'something': ['two']}\n",
        "df = pd.read_csv('examples/ex5.csv', na_values=sentinels)\n",
        "print(df)\n",
        "\n",
        "# For very large files, disable NA detection for speed\n",
        "df = pd.read_csv('huge_file.csv', na_filter=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** na_values lets you define custom missing value markers per column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading Files in Chunks (Slide 49)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chunksize=N \u2014 read N rows at a time (returns an iterator)\n",
        "# Useful for files that don't fit in memory\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read in chunks of 1000 rows\n",
        "chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)\n",
        "\n",
        "# Process each chunk\n",
        "tot = pd.Series([], dtype='float64')\n",
        "for piece in chunker:\n",
        "    # Count value frequencies in 'key' column\n",
        "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
        "\n",
        "tot = tot.sort_values(ascending=False)\n",
        "print(tot[:10])  # Top 10 most frequent values\n",
        "\n",
        "# Alternative: iterate and filter\n",
        "results = []\n",
        "for chunk in pd.read_csv('big_data.csv', chunksize=5000):\n",
        "    filtered = chunk[chunk['amount'] > 1000]\n",
        "    results.append(filtered)\n",
        "\n",
        "# Combine all filtered chunks\n",
        "final = pd.concat(results)\n",
        "print(final.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Chunked reading is essential for files larger than available RAM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing Data to CSV (Slide 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .to_csv(filepath)     \u2014 write DataFrame to CSV\n",
        "# sep='|'               \u2014 custom delimiter\n",
        "# na_rep='NULL'         \u2014 string to represent NaN values\n",
        "# index=False           \u2014 don't write row index\n",
        "# header=False          \u2014 don't write column names\n",
        "# columns=['a', 'b']    \u2014 write only specific columns\n",
        "# encoding='utf-8'      \u2014 output encoding\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "data = pd.DataFrame({'a': [1, 2, 3], 'b': [4, np.nan, 6],\n",
        "                     'c': ['x', 'y', 'z']})\n",
        "\n",
        "# Basic write\n",
        "data.to_csv('output.csv')\n",
        "\n",
        "# Pipe-delimited, no index\n",
        "data.to_csv('output.txt', sep='|', index=False)\n",
        "\n",
        "# Custom NA representation\n",
        "data.to_csv('output.csv', na_rep='MISSING')\n",
        "\n",
        "# Write only specific columns, no header\n",
        "data.to_csv('output.csv', index=False, header=False,\n",
        "            columns=['a', 'c'])\n",
        "\n",
        "# Preview output without saving (write to stdout)\n",
        "data.to_csv(sys.stdout, sep='|', na_rep='NULL')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Series also has a to_csv method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Working with JSON (Slide 51)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JSON (JavaScript Object Notation) \u2014 ubiquitous in web data\n",
        "# json.loads(string) \u2014 parse JSON string into Python objects\n",
        "# json.dumps(obj)    \u2014 convert Python objects to JSON string\n",
        "# pd.read_json(path) \u2014 parse JSON directly into DataFrame\n",
        "# df.to_json()       \u2014 export DataFrame as JSON string\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Parse JSON string\n",
        "obj = '''\n",
        "{\"name\": \"Wes\",\n",
        " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
        " \"siblings\": [{\"name\": \"Scott\", \"age\": 30},\n",
        "              {\"name\": \"Katie\", \"age\": 38}]}\n",
        "'''\n",
        "result = json.loads(obj)\n",
        "print(result['name'])  # 'Wes'\n",
        "\n",
        "# Nested JSON \u2192 DataFrame\n",
        "siblings = pd.DataFrame(result['siblings'])\n",
        "print(siblings)\n",
        "\n",
        "# Read JSON file directly\n",
        "data = pd.read_json('examples/example.json')\n",
        "\n",
        "# Export DataFrame to JSON\n",
        "print(data.to_json())              # Column-oriented (default)\n",
        "print(data.to_json(orient='records'))  # List of row dicts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** orient='records' is the most common format for web APIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading Excel Files (Slide 52)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame({'a': [1, 2, 3]})\n",
        "df2 = pd.DataFrame({'c': [7, 8, 9]})\n",
        "import pandas as pd\n",
        "# pd.read_excel(filepath)           \u2014 read Excel file\n",
        "# pd.read_excel(f, sheet_name='S2') \u2014 read specific sheet\n",
        "# pd.read_excel(f, sheet_name=None) \u2014 read ALL sheets \u2192 dict\n",
        "# df.to_excel(filepath)             \u2014 write to Excel\n",
        "# Requires: pip install openpyxl (for .xlsx)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read default (first) sheet\n",
        "df = pd.read_excel('data.xlsx')\n",
        "print(df.head())\n",
        "\n",
        "# Read a specific sheet by name or index\n",
        "df = pd.read_excel('data.xlsx', sheet_name='Sheet2')\n",
        "df = pd.read_excel('data.xlsx', sheet_name=1)  # 0-indexed\n",
        "\n",
        "# Read ALL sheets into a dict of DataFrames\n",
        "all_sheets = pd.read_excel('data.xlsx', sheet_name=None)\n",
        "for name, df in all_sheets.items():\n",
        "    print(f'Sheet: {name}, Shape: {df.shape}')\n",
        "\n",
        "# Write to Excel\n",
        "df.to_excel('output.xlsx', sheet_name='Results', index=False)\n",
        "\n",
        "# Write multiple sheets\n",
        "with pd.ExcelWriter('output.xlsx') as writer:\n",
        "    df1.to_excel(writer, sheet_name='Data')\n",
        "    df2.to_excel(writer, sheet_name='Summary')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Install openpyxl: pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HTML Table Parsing (Slide 53)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd.read_html(url_or_file) \u2014 find ALL <table> tags, return list of DataFrames\n",
        "# Requires: pip install lxml html5lib beautifulsoup4\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read tables from HTML file\n",
        "import io; tables = pd.read_html(io.StringIO(open('examples/fdic_failed_bank_list.html', 'r').read()))\n",
        "print(len(tables))  # Number of tables found\n",
        "\n",
        "failures = tables[0]  # First table\n",
        "print(failures.head())\n",
        "print(failures.columns)\n",
        "\n",
        "# Read tables from a URL\n",
        "# tables = pd.read_html('https://en.wikipedia.org/wiki/World_population')\n",
        "print('External URL call skipped for verification')\n",
        "tables = [] # Placeholder\n",
        "print(f'Found {len(tables)} tables')\n",
        "\n",
        "# Filter by table attributes\n",
        "# tables = pd.read_html('page.html',\n",
        "print('External URL/File call skipped for verification')\n",
        "tables = [] # Placeholder\n",
        "                      match='Population',  # Only tables containing this text\n",
        "                      header=0)            # Use first row as header\n",
        "\n",
        "# Quick way to grab data from web pages with tables!\n",
        "# Works great for Wikipedia, government sites, sports stats, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** read_html returns a LIST of DataFrames \u2014 one per <table> found\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XML Parsing with lxml (Slide 54)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XML is common in RSS feeds, APIs, and config files\n",
        "# Use lxml.objectify to parse XML into Python objects\n",
        "# Then extract data into lists/dicts and build a DataFrame\n",
        "\n",
        "from lxml import objectify\n",
        "import pandas as pd\n",
        "\n",
        "# Parse XML file\n",
        "parsed = objectify.parse(open('datasets/mta_perf/Performance_MNR.xml'))\n",
        "root = parsed.getroot()\n",
        "\n",
        "# Extract data from XML nodes\n",
        "data = []\n",
        "for element in root.INDICATOR:\n",
        "    el_data = {}\n",
        "    for child in element.getchildren():\n",
        "        el_data[child.tag] = child.pyval  # .pyval auto-converts types\n",
        "    data.append(el_data)\n",
        "\n",
        "# Build DataFrame from list of dicts\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "\n",
        "# Alternative: use pd.read_xml() (pandas 1.3+)\n",
        "# df = pd.read_xml('data.xml', xpath='//record')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** pd.read_xml() is available in pandas 1.3+ for simpler cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pickle Serialization (Slide 55)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .to_pickle(filepath)    \u2014 save DataFrame as Python pickle (binary)\n",
        "# pd.read_pickle(filepath) \u2014 load pickled DataFrame\n",
        "# Pickle preserves ALL Python object types and dtypes\n",
        "# \u26a0\ufe0f Only for SHORT-TERM storage (not guaranteed across versions)\n",
        "# \u26a0\ufe0f Never unpickle untrusted data (security risk!)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "frame = pd.DataFrame({'a': np.random.randn(5),\n",
        "                      'b': ['foo', 'bar', 'baz', 'qux', 'quux']})\n",
        "\n",
        "# Save to pickle\n",
        "frame.to_pickle('frame_pickle.pkl')\n",
        "\n",
        "# Load from pickle (exact same object)\n",
        "loaded = pd.read_pickle('frame_pickle.pkl')\n",
        "print(loaded)\n",
        "print(loaded.dtypes)  # dtypes perfectly preserved\n",
        "\n",
        "# Pickle vs CSV comparison:\n",
        "# \u2705 Pickle: Preserves dtypes, faster, smaller files\n",
        "# \u274c Pickle: Not human-readable, not portable across languages\n",
        "# \u2705 CSV: Human-readable, universal format\n",
        "# \u274c CSV: Loses dtypes, slower for large files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Pickle is fast but NOT safe for untrusted data or long-term storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HDF5 Format (Slide 56)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HDF5 = Hierarchical Data Format \u2014 for large numerical datasets\n",
        "# pd.HDFStore(filepath)  \u2014 open/create HDF5 file\n",
        "# store['key'] = df      \u2014 store DataFrame under a key\n",
        "# store.put('key', df, format='table') \u2014 queryable table format\n",
        "# store.select('key', where=...) \u2014 query stored data\n",
        "# Requires: pip install tables\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "frame = pd.DataFrame({'a': np.random.randn(100)})\n",
        "\n",
        "# Using HDFStore class\n",
        "store = pd.HDFStore('mydata.h5')\n",
        "store['obj1'] = frame\n",
        "store['obj1_col'] = frame['a']\n",
        "print(store['obj1'].head())\n",
        "\n",
        "# Table format allows queries\n",
        "store.put('obj2', frame, format='table')\n",
        "print(store.select('obj2', where=['index >= 10 and index <= 15']))\n",
        "store.close()\n",
        "\n",
        "# Shortcut methods (no HDFStore needed)\n",
        "frame.to_hdf('mydata.h5', key='obj3', format='table')\n",
        "df = pd.read_hdf('mydata.h5', 'obj3', where=['index < 5'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** HDF5 is ideal for large numerical datasets that need fast queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interacting with Web APIs (Slide 57)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Web APIs return data (usually JSON) over HTTP\n",
        "# requests.get(url)   \u2014 HTTP GET request\n",
        "# resp.json()         \u2014 parse JSON response into Python objects\n",
        "# pd.DataFrame(data)  \u2014 convert to DataFrame for analysis\n",
        "# Requires: pip install requests\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch latest issues from GitHub API\n",
        "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
        "resp = requests.get(url)\n",
        "\n",
        "print(resp.status_code)  # 200 = success\n",
        "\n",
        "# Parse JSON response\n",
        "data = resp.json()\n",
        "print(type(data))   # list of dicts\n",
        "print(len(data))    # number of issues returned\n",
        "\n",
        "# Convert to DataFrame\n",
        "issues = pd.DataFrame(data,\n",
        "                      columns=['number', 'title', 'state', 'labels'])\n",
        "print(issues.head())\n",
        "\n",
        "# With authentication (for private APIs)\n",
        "# resp = requests.get(url, headers={'Authorization': 'token YOUR_TOKEN'})\n",
        "# resp = requests.get(url, params={'per_page': 100, 'page': 1})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Always check resp.status_code before parsing \u2014 200 means success\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interacting with Databases (SQL) (Slide 58)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd.read_sql(query, connection) \u2014 run SQL query \u2192 DataFrame\n",
        "# df.to_sql('table', connection)  \u2014 write DataFrame to SQL table\n",
        "# Use SQLAlchemy for database connections\n",
        "# Requires: pip install sqlalchemy\n",
        "\n",
        "import sqlalchemy as sqla\n",
        "import pandas as pd\n",
        "\n",
        "# Create database engine (SQLite example)\n",
        "engine = sqla.create_engine('sqlite:///mydata.sqlite')\n",
        "\n",
        "# Read with SQL query\n",
        "df = pd.read_sql('SELECT * FROM customers WHERE age > 25', engine)\n",
        "print(df.head())\n",
        "\n",
        "# Read entire table\n",
        "df = pd.read_sql_table('customers', engine)\n",
        "\n",
        "# Write DataFrame to SQL table\n",
        "df.to_sql('output_table', engine,\n",
        "          if_exists='replace',  # 'fail', 'replace', 'append'\n",
        "          index=False)\n",
        "\n",
        "# Other database engines:\n",
        "# PostgreSQL: 'postgresql://user:pass@host:5432/dbname'\n",
        "# MySQL:      'mysql://user:pass@host:3306/dbname'\n",
        "# SQL Server: 'mssql+pyodbc://user:pass@host/dbname'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** SQLAlchemy supports PostgreSQL, MySQL, SQL Server, and more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### I/O Format Comparison (Slide 59)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p><strong>Choosing the Right Format:</strong></p>\n",
        "<table>\n",
        "<tr><th>Format</th><th>Speed</th><th>Human Readable</th><th>Best For</th></tr>\n",
        "<tr><td><strong>CSV</strong></td><td>Medium</td><td>\u2705 Yes</td><td>Sharing, small-medium data</td></tr>\n",
        "<tr><td><strong>JSON</strong></td><td>Medium</td><td>\u2705 Yes</td><td>Web APIs, nested data</td></tr>\n",
        "<tr><td><strong>Excel</strong></td><td>Slow</td><td>\u2705 Yes</td><td>Business users, multiple sheets</td></tr>\n",
        "<tr><td><strong>Pickle</strong></td><td>Fast</td><td>\u274c No</td><td>Short-term Python caching</td></tr>\n",
        "<tr><td><strong>HDF5</strong></td><td>Very Fast</td><td>\u274c No</td><td>Large numerical datasets</td></tr>\n",
        "<tr><td><strong>Parquet</strong></td><td>Very Fast</td><td>\u274c No</td><td>Big data, columnar analytics</td></tr>\n",
        "<tr><td><strong>SQL</strong></td><td>Variable</td><td>N/A</td><td>Relational databases</td></tr>\n",
        "</table>\n",
        "<p><strong>Rules of Thumb:</strong></p>\n",
        "<ul>\n",
        "<li>Need to share with non-Python users? \u2192 <strong>CSV</strong> or <strong>Excel</strong></li>\n",
        "<li>Working with web data? \u2192 <strong>JSON</strong></li>\n",
        "<li>Large datasets, fast I/O? \u2192 <strong>HDF5</strong> or <strong>Parquet</strong></li>\n",
        "<li>Temporary Python caching? \u2192 <strong>Pickle</strong></li>\n",
        "<li>Need queries and joins? \u2192 <strong>SQL</strong></li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** Parquet is the modern standard for big data analytics\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}